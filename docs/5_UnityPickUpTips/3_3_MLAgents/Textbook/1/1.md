1.　ポリシーについて


+ <b>ポリシー</b>  
    　
強化学習では　現在の「状態」に応じて、次の「行動」を決定します。  
この次の「行動」を決定するための戦略。  
具体的には「ある状態である行動を行う確率」をポリシー（方策）と呼びます。  



ML-Agentsにおける「ポリシー（Policy）」とは、エージェントがどのように行動するかを決定するルールやアルゴリズムのことを指します。具体的には、エージェントが環境における観測（Observations）を受け取ったとき、それに基づいてどの行動（Actions）を選択するかを定義するものです。

強化学習において、ポリシーは通常、次の2つの方法で表現されます。

### 1. **決定論的ポリシー（Deterministic Policy）**
   - 特定の観測に対して常に同じ行動を選択します。  
   - 例えば、「観測Aのときは常に行動Xをとる」といった単純な規則に基づいて行動が選ばれます。
   - 決定論的ポリシーは、エージェントの行動が確定的である場合に使用されますが、学習の初期段階では探索の幅が狭くなりがちです。

### 2. **確率的ポリシー（Stochastic Policy）**
   - 特定の観測に対して複数の行動の中から確率的に選択します。  
   - 例えば、「観測Aのときには80%の確率で行動X、20%の確率で行動Yを選ぶ」というように、行動が確率的に決まる仕組みです。
   - 確率的ポリシーは、探索と利用のバランスをとるのに適しています。探索（未踏の領域を試すこと）を行いつつ、すでに良いと分かっている行動（利用）も取るため、強化学習でよく使われます。

### ML-Agentsにおけるポリシーの動作
ML-Agentsでは、エージェントは**ニューラルネットワーク**をポリシーとして使用することが一般的です。これにより、エージェントが観測した環境の情報を元に、最適な行動を選択する方法を学習します。ニューラルネットワークのトレーニングは強化学習アルゴリズム（例: PPO）によって行われます。

#### ポリシーの役割
1. **観測の処理**:
   - ポリシーはエージェントがセンサから得たデータ（観測値）を入力として受け取ります。観測値はエージェントが環境をどのように認識しているかを示します。
   
2. **行動の決定**:
   - ポリシーは、観測値を基にして行動を出力します。これらの行動は、エージェントが環境内でどのように移動するかや、他のエージェントとのインタラクションに影響を与えます。

3. **学習と改善**:
   - エージェントが行動を実行すると報酬を得ます。ポリシーは、その報酬を基に行動の選択を改善していきます。この過程は「強化学習」と呼ばれ、エージェントがより高い報酬を得るためにポリシーが学習されていきます。

#### ポリシーの学習
- **PPO（Proximal Policy Optimization）**などのアルゴリズムを使って、ポリシーがエージェントの学習中に徐々に改善されます。これにより、エージェントは、トライ・アンド・エラーを繰り返しながら、最適な行動を見つけるように進化します。

#### ポリシータイプ
ML-Agentsでは以下のポリシーが選べます：
1. **学習ポリシー**:
   - ニューラルネットワークをトレーニングし、環境から学び続けるポリシー。
   
2. **ヒューリスティックポリシー**:
   - 人間が決定したルールに基づいて行動を選択するモード（人間が操作する場合など）。

---

### 実装例
例えば、ML-Agentsでの実装では、次のようにポリシーを設定します：

```csharp
BehaviorParameters behaviorParameters = GetComponent<BehaviorParameters>();
behaviorParameters.BehaviorType = BehaviorType.Default; // 学習ポリシー
```

ポリシーはML-Agentsを使ったエージェントのコア要素であり、エージェントがどのように学習し、行動を最適化するかを司ります。

<br>

---

<br>

# ポリシー更新

**ポリシー更新**とは、エージェントが環境から得た経験に基づいて、エージェントの行動選択を改善するためにポリシー（行動ルールや方針）を調整するプロセスのことです。具体的には、エージェントが強化学習のトレーニングを通じて「より良い」行動を学習するために、ニューラルネットワークの重み（パラメータ）を更新します。

### ポリシー更新の流れ
強化学習におけるポリシー更新の主なステップは次の通りです。

1. **環境の観測と行動の選択**
   - エージェントは環境を観測し（観測データ）、その観測に基づいてポリシーが行動を決定します。この行動はポリシー、通常はニューラルネットワークを使って選ばれます。

2. **環境からのフィードバック（報酬）**
   - エージェントが行動を実行すると、環境からフィードバックとして「報酬」を得ます。この報酬は、行動の結果としてエージェントがどれだけうまくいったかを示す指標です。
   - ポジティブな報酬（例: ゴールに近づいた）は、行動が良かったことを示し、ネガティブな報酬（例: 崖から落ちた）は行動が悪かったことを示します。

3. **経験の蓄積**
   - エージェントは、観測、行動、報酬、および新しい状態のデータを「経験」としてメモリ（経験バッファ）に蓄積します。この経験を元に、後でポリシーを更新します。

4. **勾配降下法によるポリシー更新**
   - 蓄積された経験データを使って、ニューラルネットワークの重み（パラメータ）を更新します。これにより、次に同じ観測が与えられたときに、エージェントがより良い行動を選択できるようになります。
   - 具体的には、**損失関数**（エージェントが得る報酬を最大化するためにポリシーをどの程度改善する必要があるかを表す関数）を最小化するために、ニューラルネットワークの重みを調整します。このプロセスには**勾配降下法**が使われます。
   
   - 強化学習では、通常次の2つの要素に基づいてポリシー更新が行われます:
     - **ポリシー勾配**: 行動がどれくらい報酬に貢献したかを反映して、行動選択の方針を調整します。
     - **価値関数の推定**: エージェントがどれくらいの報酬を期待できるかを学習し、将来の報酬に基づいて行動を評価します。

### ML-Agentsにおけるポリシー更新の仕組み
ML-Agentsでは、**PPO（Proximal Policy Optimization）**というアルゴリズムが使われており、エージェントが効果的に学習できるようにポリシー更新を行います。

#### 1. **経験の収集**
   - エージェントは、複数のエピソードをプレイし、そこで観測、行動、報酬のトリプルを記録します。これらは「経験バッファ」に格納されます。

#### 2. **PPOによるポリシー更新**
   PPOでは、次の要素に基づいてポリシーを更新します：
   
   - **クリッピングされたポリシー更新**: PPOは大幅なポリシーの変更を避けるために、ポリシーの更新が急激になりすぎないよう制約を設けています。これにより、安定して学習を進めることができます。
   - **アドバンテージ推定**: アドバンテージ関数を用いて、どの行動がどれほど価値があったかを評価し、その情報を基にポリシーを改善します。

#### 3. **ニューラルネットワークのトレーニング**
   - 収集した経験に基づいて、ニューラルネットワークのパラメータ（重み）を更新します。これにより、次に同じ状況が発生した場合、エージェントはより良い行動を選択する可能性が高くなります。

#### 4. **エージェントの学習の進行**
   - 学習が進むにつれて、エージェントは繰り返し環境から学び、ポリシー更新を行い、最終的に環境に対して最適な行動を選択できるようになります。

---

### ポリシー更新の要点
- **ポリシー更新**は、エージェントが観測に基づいてより良い行動を選択するようにニューラルネットワークの重みを調整するプロセスです。
- 強化学習では、エージェントが得た報酬を最大化するために、ポリシーを改善する方法を学習します。
- ML-Agentsでは、**PPO**が主なアルゴリズムとして使われており、これによりエージェントは安定的に学習を進めます。