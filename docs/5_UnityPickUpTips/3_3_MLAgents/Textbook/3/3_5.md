# 3-5 学習設定ファイル

2の「学習と推論」での設定について

ここで紹介するもの以外に「セルフプレイ」「Curiosity」「模倣学習」「LSTM」専用のパラメータも存在しますがそちらは別で言及します

<br>

## 学習設定ファイルとは

「学習設定ファイル」(*.yaml)は、学習に利用するハイパーパラメータを設定するファイルです。  
機械学習のパラメータの中で、人間が調整sする必要があるパラメータのことを「ハイパーパラメータ」と呼びます。  
学習設定ファイルは１環境につき１つ用意する必要があります。

### PPOとSAC

「Unity ML-Agents」で標準で使える強化学習アルゴリズムは「PPO」と「SAC」になります。
ハイパーパラメーターの設定項目は、「PPO」と「SAC」どちらのアルゴリズムを使うかで異なります。
慣れないうちはPPOを使うことで問題ありません。

PPOとSAOの違いは[別の章](../4/4_1.md)で解説します

```
・PPO（Proximal Policy Optimization）
・SAC（Soft Actor-Critic）
```


<br>

## PPOの学習設定ファイルの例

「PPO」の学習設定ファイルの例は、次の通りです。  
最上位セクション「behaviors:」の下にセクション「<Behavior Name>:」を配置し、さらにその下に各種ハイパーパラメーターを設定します。「Behavior Name」は、「Behavior Parameters」で設定するポリシー毎の識別子になります。

```yaml
behaviors:
  RollerBall:
    #トレーナー識別
    trainer_type: ppo 
    
    #基本
    max_steps: 500000
    time_horizon: 64
    summary_freq: 1000
    keep_checkpoints: 5

    #学習アルゴリズム
    hyperparameters:
      #PPOとSAC共通
      batch_size: 10
      buffer_size: 100
      learning_rate: 0.0003
      learning_rate_schedule: linear

      #PPO用
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
    
    #ニューラルネットワーク
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2

    #報酬
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

```


<br>

## SACの学習設定ファイルの例
「SAC」の学習設定ファイルの例は、次のとおりです。「トレーナー種別」と「ハイパーパラメーター SAC用」以外の設定は、「PPO」と同じ設定項目になります。

```yaml
behaviors:
  #トレーナー識別  
  RollerBall:
    trainer_type: sac

    max_steps: 500000
    time_horizon: 64
    summary_freq: 1000
    keep_checkpoints: 5

    hyperparameters:
      batch_size: 64
      buffer_size: 12000
      learning_rate: 0.0003
      learning_rate_schedule: constant

      #SAC専用
      buffer_init_steps: 0
      tau: 0.005
      steps_per_update: 10.0
      save_replay_buffer: false
      init_entcoef: 0.01
      reward_signal_steps_per_update: 10.0

    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0


```


<br>

## `behaviors` セクション
- `RollerBall`: これはエージェントのビヘイビア名です。ML-Agentsでエージェントに割り当てる名前で、スクリプトと一致する必要があります。

<br>

## トレーナーの種別
#### `trainer_type`
学習アルゴリズム（デフォルト：ppo）

<br>

## 基本のハイパーパラメーター

基本のパラメーターは次の通りです。
#### `max_steps`
- エージェントが学習を終了するまでの最大ステップ数。(デフォルト:500000)  
複雑な問題の場合、この値を増やす必要があります。  
例えば `5e6` は500万ステップを意味します。  

#### `time_horizon`
- 経験バッファに追加する前に、エージェント毎に収集する経験の数(デフォルト：64)。
PPOでは、エピソード内で頻繁に報酬が与えられる場合は小さな値。そうでない場合は大きい値が良いです。エージェントの行動シーケンス内の重要な動作をキャプチャするのに十分な大きさでなければなりません。    
SACでは、このパラメータはさほど重要ではなく、通常はおおよそのエピソード長を設定すれば問題ありません。
  
(エージェントが経験を蓄積するためにどのくらいの時間を考慮するか。これを小さくすると短期的な動作に対する反応が強化され、大きくすると長期的な報酬に対する考慮が強化されます。)

<br>

#### `summary_freq`
- 統計情報を何ステップ毎に保存するか。学習の進行状況が出力される頻度（デフォルト：5000ステップ）。このステップ数ごとに進行状況が記録されます。TensorBordで表示するグラフの粒度も変わります。

<br>

#### `checkpoint_interval`
- チェックポイントを何経験(何ステップ)ごとに保存するか（デフォルト：500000）。トレーニング中にモデルの状態を保存する頻度を指定するパラメータです。トレーニングが進行中でも、途中の状態を保存しておくことで、後からその状態に戻ってトレーニングを再開したり、異なるモデルバージョンを評価したりすることができます

<br>

#### `keep_checkpoints`
- 保存するチェックポイントの最大数（デフォルト：５）。最大数に達すると、古いチェックポイントから削除されます。

<br>

#### `threaded`
- 並列実行を有効にするかどうか。複数のスレッドでトレーニングを行う場合に`true`にします。（デフォルト：false）。ただ、trueにすると、環境ステップ実行中にポリシーの更新が行われる可能性があります。これは、学習の高速化と引き換えに、PPOのポリシー上の想定に少し違反します。

<br>

#### `init_path`
- 既存モデルで初期化して新規学習を開始するためのパス（デフォルト：None）。  
**事前に学習済みのモデル（チェックポイント）を読み込むパス**を指定するために以下の時に使います。

- **学習の再開**: 
  トレーニングを一から始めるのではなく、既に学習済みのモデルを利用してトレーニングを再開したい場合に使用します。例えば、トレーニングが途中で中断されたり、さらに学習を進めたい場合に、以前保存したチェックポイントからトレーニングを続けることができます。すでに学習済みのモデルを利用して新しいタスクに応用できたりします。   
  ゼロから再学習する必要がなく、効率的です。

- **ファインチューニング**:
  あるタスクに対して事前に学習させたモデルを、別のタスクに転移学習させたい場合にも使えます。  
  既存の学習済みモデルに新しいデータや条件を加えて、さらなる学習や性能向上を目指す際に便利です。  
  すでに学習された知識を活用して新しいタスクのトレーニングを加速できます。

### **設定例**

以下は、`init_path` の設定例です。

```yaml
init_path: "results/PreviousTraining/checkpoint-10000"
```

- `results/PreviousTraining/checkpoint-10000` は、以前のトレーニング結果が保存されているディレクトリ内にある `checkpoint-10000` というチェックポイントを指します。
- この設定により、トレーニングは `checkpoint-10000` から始まり、続きのトレーニングを行います。

### **注意点**

- **互換性の確認**: `init_path` を使ってモデルを再開する際、再開するタスクや環境が以前のタスクと大きく異なる場合、期待した結果が得られないことがあります。例えば、環境の設定や報酬構造が大きく異なると、学習した内容が適応できないことがあります。
  
- **パスの正確な指定**: `init_path` に指定するパスが正しくない場合、エラーが発生してトレーニングが開始できません。保存したチェックポイントの正しいパスを指定することが重要です。

<br>

## 学習アルゴリズム

「hyperparameters」下に、学習アルゴリズムのパラメータを設定します。  
学習アルゴリズムのパラメータは「PPOとSAC共通」「PPO用」「SAC用」の３種類に分けられます

### PPOとSAC共通

- **`batch_size`**: 勾配降下１回に使用する経験数。バッチごとの学習サンプルの数。大きいほど安定するが、学習に時間がかかる可能性があります。

+ 
+ 
+ 

<br>

- **`buffer_size`**: 一度にバッファに蓄積される経験の総数。バッチを作成するために使用されます。

+
+

<br>

- **`learning_rate`**: 学習率。大きいほど速く学習するが、不安定になる可能性があり、小さすぎると収束が遅くなります。

+ 

<br>



- **`learning_rate_schedule`**: 学習率が時間とともにどのように変わるかを設定。ここでは線形的に減少する設定です。


<br>


### PPO用

- **`beta`**: エントロピー正則化係数。エージェントの行動のランダム性を維持するためのもの。小さくすると決定的な行動を取るようになります。

・


<br>

- **`epsilon`**: PPO特有のクリップ範囲。大きいほど、更新が行動ポリシーから外れることを許可しますが、不安定になる可能性があります。

・


<br>


- **`lambd`**: GAE（Generalized Advantage Estimation）に使われるパラメータ。将来の報酬予測に関与します。

・


<br>

- **`num_epoch`**: 学習する際のバッファからバッチをどれだけ繰り返して使用するかの回数。

・



<br>



### SAC用

- **``**:
- ・

- **``**:
- ・

- **``**:
- ・

- **``**:
- ・

- **``**:
- ・

- **``**:
- ・










## ニューラルネットワーク
`network_settings`（ネットワーク設定）下に、ニューラルネットワークのパラメータを設定します。

- **`normalize`**: 観察データを正規化するかどうか。学習を安定させるために`true`にするのが一般的です。

<br>

- **`hidden_units`**: 各レイヤーの隠れユニット数。値が大きいほど複雑なパターンを学習できるが、学習時間が増えます。


<br>


- **`num_layers`**: ニューラルネットワークのレイヤー数。


<br>


- **`vis_encode_type`**: ビジュアル観察のエンコーダーの種類（もし視覚入力がある場合）。



- **`conditioning_type`**: 



<br>

## 環境報酬
`reward_signals`（報酬信号）下の `extrinsic`(外的報酬。環境から与えられる報酬の設定)下に、環境報酬のパラメータを設定します。

  - **`strength`**: この報酬信号の強さを設定。

<br>

  - **`gamma`**: 割引率。将来の報酬の重要度をどれだけ考慮するか。


<br>

---
---

<br>

# 勾配降下

「勾配降下」は損失（最適解との差）を最小にするパラメータ（重みやバイアス）を見つけるためのアルゴリズムです。このアルゴリズムを使って、より多くの報酬がもらえる行動をとるように、ポリシーの更新を行います。  
「勾配降下」の処理の流れは次のとおりです。

①　パラメータを適当な位置で初期化

②　与えられたパラメータにおける損失（最適解との差）を表すグラフの傾きを計算

③　最も傾きの大きい方向に、パラメータを少しずらす

④　②と③を繰り返す

これによって、ボールが勾配（斜面）を転がっていくようにパラメータを最適解に近づけていきます。以下は、縦軸が損失、横軸がパラメータの値を示すグラフになります。

「勾配降下」に関するハイパーパラメータは、  
「batch_size」  
「num_epoch」(PPO)  
「epsilon」(PPO)    
の３つがあります。  
「batch_size」は１回の勾配降下で利用する学習データ（経験）の数、  
「num_epoch」(PPO)は１回のポリシー更新時で行う勾配降下の回数、   
「epsilon」(PPO) は旧ポリシーと新ポリシーの更新比率に対する許容限界を指定します。

<img src="images/3_5_1.JPG" width="90%" alt="" title="">

<br>

---
---

<br>


# num_layers と　hidden_units
「num_layers」はニューラルネットワークの隠れ層の数、「hidden_units」は隠れ層のニューロン(ユニット)の数です

<img src="images/3_5_2.JPG" width="90%" alt="" title="">

<br>

---
---

<br>

# time_horizon　と　buffer_size

「time_horizon」と「buffer_size」はエージェントの経験（観察、行動、報酬）の収集に関するハイパーパラメータです。  
各エージェントは、経験を「time_horizon」分収集したら学習アルゴリズムの経験バッファに積み、経験バッファは経験を「buffer_size」分収集したら「ポリシー更新」を行います。

<img src="images/3_5_3.JPG" width="90%" alt="" title="">


<br>

---
---

<br>




# ・ハイパーパラメーターの用途別まとめ

「学習設定ファイル」のたくさんのハイパーパラメータを紹介しましたが、


① **学習量が足りない**  
対処：学習ステップ数を増やす  
パラメータ：max_steps

② **統計情報の保存頻度を更新したい**  
対処：統計情報の保存頻度の設定  
パラメータ：summary_freq

③ **行動種別(ContinuousとDiscrete)に応じて調整する**   
対処：バッチサイズの調整  
パラメータ：batch_size、buffer_size、time_horizon  
対処：正規化の調整  
パラメータ：normalize


④ **エージェントが問題を理解できていない**  
対処：ニューラルネットワークのサイズの調整  
パラメータ：num_layers、hidden_units、vis_encode_type(Visual Observation)

⑤ **エージェントが探索しすぎ、しなさすぎ**  
対処：エントロピー（行動のランダムさ）の調整  
パラメータ：beta(PPO)、init_entcoef(SAC)  
対処：学習率(探索と活用の割合)の調整   
パラメータ：learning_rate、learning_rate_schedule  

<br>


<br>

これらの設定を調整することで、エージェントの学習速度やパフォーマンスをコントロールできます。




















