# 3-5 学習設定ファイル

2の学習と推論でも設定について

ここで紹介するもの以外に「セルフプレイ」「Curiosity」「模倣学習」「LSTM」専用のパラメータも存在しますがそちらは４で言及します


## 学習設定ファイルとは

「学習設定ファイル」(*.yaml)は、学習に利用するハイパーパラメータを設定するファイルです。  
機械学習のパラメータの中で、人間が調整sする必要があるパラメータのことを「ハイパーパラメータ」と呼びます。  
学習設定ファイルは１環境につき１つ用意する必要があります。

### PPOとSAC

「Unity ML-Agents」で標準で使える強化学習アルゴリズムは「PPO」と「SAC」になります

SAC

基本的にPPO











#### トレーナーの種別



#### ハイパーパラメーター


#### 学習アルゴリズム

##### PPOとSAC共通


#### ニューラルネットワーク

勾配降下







PPO（Proximal Policy Optimization）は、ML-Agentsでよく使用される強化学習アルゴリズムです。PPOの学習設定ファイル（YAMLファイル）の例とその解説を以下に示します。

### PPOの学習設定ファイルの例（YAML形式）
```yaml
behaviors:
  TestAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 64
      buffer_size: 2048
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 5e6
    time_horizon: 64
    summary_freq: 10000
    threaded: true
```

### 解説
#### `behaviors` セクション
- `TestAgent`: これはエージェントのビヘイビア名です。ML-Agentsでエージェントに割り当てる名前で、スクリプトと一致する必要があります。

#### `trainer_type`
- `ppo`: 使用するトレーナーの種類を指定します。ここではPPOを使用しています。

#### `hyperparameters`（ハイパーパラメータ）
- **`batch_size`**: バッチごとの学習サンプルの数。大きいほど安定するが、学習に時間がかかる可能性があります。
- **`buffer_size`**: 一度にバッファに蓄積される経験の総数。バッチを作成するために使用されます。
- **`learning_rate`**: 学習率。大きいほど速く学習するが、不安定になる可能性があり、小さすぎると収束が遅くなります。
- **`beta`**: エントロピー正則化係数。エージェントの行動のランダム性を維持するためのもの。小さくすると決定的な行動を取るようになります。
- **`epsilon`**: PPO特有のクリップ範囲。大きいほど、更新が行動ポリシーから外れることを許可しますが、不安定になる可能性があります。
- **`lambd`**: GAE（Generalized Advantage Estimation）に使われるパラメータ。将来の報酬予測に関与します。
- **`num_epoch`**: 学習する際のバッファからバッチをどれだけ繰り返して使用するかの回数。
- **`learning_rate_schedule`**: 学習率が時間とともにどのように変わるかを設定。ここでは線形的に減少する設定です。

#### `network_settings`（ネットワーク設定）
- **`normalize`**: 観察データを正規化するかどうか。学習を安定させるために`true`にするのが一般的です。
- **`hidden_units`**: 各レイヤーの隠れユニット数。値が大きいほど複雑なパターンを学習できるが、学習時間が増えます。
- **`num_layers`**: ニューラルネットワークのレイヤー数。
- **`vis_encode_type`**: ビジュアル観察のエンコーダーの種類（もし視覚入力がある場合）。

#### `reward_signals`（報酬信号）
- **`extrinsic`**: 外的報酬。環境から与えられる報酬を設定します。
  - **`gamma`**: 割引率。将来の報酬の重要度をどれだけ考慮するか。
  - **`strength`**: この報酬信号の強さを設定。

#### `max_steps`
- エージェントが学習を終了するまでの最大ステップ数。例えば `5e6` は500万ステップを意味します。

#### `time_horizon`
- エージェントが経験を蓄積するためにどのくらいの時間を考慮するか。これを小さくすると短期的な動作に対する反応が強化され、大きくすると長期的な報酬に対する考慮が強化されます。

#### `summary_freq`
- 学習の進行状況が出力される頻度（ステップ数単位）。例えば10000ステップごとに進行状況が記録されます。

#### `threaded`
- 並列実行を有効にするかどうか。複数のスレッドでトレーニングを行う場合に`true`にします。

これらの設定を調整することで、エージェントの学習速度やパフォーマンスをコントロールできます。















